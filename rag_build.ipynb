{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caff47a",
   "metadata": {},
   "source": [
    "### This file creates embeddings based on the given text data and saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286e6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621db037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents (chunks): 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44401/2312775883.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758172797.290475   44401 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Document ---\n",
    "# Make sure your file path is correct\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Use a Glob pattern to load all .txt files from the 'content' directory\n",
    "loader = DirectoryLoader(\n",
    "    \"./content/\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# --- 2. Split the Document (with larger chunk size) ---\n",
    "# We use a larger chunk size to get more context per document chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Total number of documents (chunks): {len(docs)}\")\n",
    "\n",
    "# --- 3. Create Embeddings and Vector Store ---\n",
    "# The embedding model converts text to numerical vectors\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store and a retriever that gets more documents\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "# We set 'k' to 10 to retrieve the top 10 most relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# --- 4. Define the RAG Prompt and Language Model ---\n",
    "# We define a custom prompt to encourage a more detailed answer\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Be very detailed and comprehensive in your answer, providing a thorough summary based on the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "load_dotenv()\n",
    "# Initialize the Gemini Language Model\n",
    "# Note: You need to set up your Google API key in Colab secrets\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7, google_api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "\n",
    "# --- 5. Build the RAG Chain ---\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895ea76",
   "metadata": {},
   "source": [
    "### Inferencing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a852b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating summary...\n",
      "\n",
      "Based on the provided text, a direct comparison of the 2017-2021 and 2025 settlements is difficult because the 2025 settlement is not fully detailed.  The documents describe the 2025 settlement as a conclusion to demands raised in 2021, 2024, and a management proposal from 2021.  The specifics of those demands and proposals are not included in this excerpt.  However, we can make some observations regarding differences based on what is available:\n",
      "\n",
      "\n",
      "**Differences that can be inferred:**\n",
      "\n",
      "* **Duration:** The 2017-2021 settlement covered a four-year period (48 months). The 2025 settlement's duration isn't explicitly stated but it's implied to be at least from 01.12.2021 to 31.12.2026 (5 years), based on the statement that the next settlement will be due from January 1st, 2027.\n",
      "\n",
      "* **Scope:** The 2017-2021 settlement applied to permanent workmen in salary/wage categories MC1 to MC6, MT1 to MT4, and G03 to G10.  The 2025 settlement also includes the resolution of demands from letters dated 17.04.2024, and 04.10.2024, suggesting that it may encompass additional issues or worker categories not addressed in the earlier agreement.  The 2025 agreement explicitly mentions Bidadi factory workers, whereas the previous settlement mentions Adugodi and Bidadi.\n",
      "\n",
      "* **Specific Provisions:** The 2017-2021 agreement includes details on general increases, service weightage, and allowances (HRA, Conveyance, Education Subsidy etc.).  The 2025 settlement details specific corrections to wage grades (G03 and G04 to G07), ABP amounts (from INR 4,080 to INR 5,818), and a significant ex-gratia payment (INR 1,13,400) for the period 01.12.2021 to 30.11.2022 for G03 to G10 category workers.  These specific financial adjustments are absent from the description of the 2017-2021 settlement, though it does mention wage increases.  The 2025 agreement also addressed issues and demands beyond the previously established terms.\n",
      "\n",
      "* **Settlement Process:** The 2025 settlement's document explicitly mentions multiple meetings and a special general body meeting held by the MEA to discuss and approve the terms. This suggests a more involved process than is described for the 2017-2021 settlement.\n",
      "\n",
      "\n",
      "**Lack of Information:**\n",
      "\n",
      "The provided text lacks the full details of the 2017-2021 settlement and the specific demands and management proposals that formed the basis of the 2025 agreement.  Therefore, a comprehensive comparison is impossible without this missing information.  The snippets provided only highlight key differences in duration, some specific financial provisions, and the overall scope of the agreements.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Invoke the Chain ---\n",
    "# The final result will be a more detailed summary\n",
    "print(\"\\nGenerating summary...\\n\")\n",
    "summary_question = \"Compare settlement of 2017-2021 to 2025 what are major changes ?\"\n",
    "result = rag_chain.invoke(summary_question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059688a",
   "metadata": {},
   "source": [
    "### SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b67ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vector store...\n",
      "Vector store saved to 'vector_db'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44401/1660703030.py:35: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def save_vector_store(persist_directory=\"vector_db\"):\n",
    "    \"\"\"\n",
    "    Loads documents, creates embeddings, and saves the vector store to a directory.\n",
    "    \"\"\"\n",
    "    print(\"Saving vector store...\")\n",
    "    \n",
    "    # 1. Load documents\n",
    "    loader = DirectoryLoader(\n",
    "        \"./content/\",\n",
    "        glob=\"*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    data = loader.load()\n",
    "\n",
    "    # 2. Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    docs = text_splitter.split_documents(data)\n",
    "\n",
    "    # 3. Create embeddings\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 4. Create and persist the vector store\n",
    "    # This automatically saves the embeddings to the specified directory\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(f\"Vector store saved to '{persist_directory}'\")\n",
    "\n",
    "# Execute the function to save the vector store\n",
    "save_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5a0ed",
   "metadata": {},
   "source": [
    "### THIS CODE IS FOR INFERENCING AFTER SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acbfc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37409/2737315069.py:26: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n",
      "E0000 00:00:1758098251.139389   37409 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- ONE-TIME SETUP ---\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define the persistence directory for your vector store\n",
    "PERSIST_DIRECTORY = \"vector_db\"\n",
    "\n",
    "# 1. Initialize the embedding model (only once)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Load the persisted vector store (only once)\n",
    "if not os.path.exists(PERSIST_DIRECTORY):\n",
    "    print(f\"Error: Vector store directory '{PERSIST_DIRECTORY}' not found.\")\n",
    "else:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "# 3. Create the retriever (only once)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 4. Define the RAG prompt (only once)\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Search for the following context and answer to the point for the given question..\n",
    "If you don't know the answer, just say that you don't know. Be very detailed and comprehensive in your answer, providing a detailed on the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 5. Initialize the LLM (only once)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.environ.get('GOOGLE_API_KEY')\n",
    ")\n",
    "\n",
    "# 6. Build the RAG chain (only once)\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# --- INFERENCE FUNCTION ---\n",
    "\n",
    "def inference_with_rag(query):\n",
    "    \"\"\"\n",
    "    Performs a RAG query using the pre-loaded chain.\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuerying: '{query}'\")\n",
    "    if 'rag_chain' in globals():\n",
    "        result = rag_chain.invoke({\"input\": query})\n",
    "        return result['answer']\n",
    "    else:\n",
    "        return \"RAG chain not initialized. Check for errors during setup.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64c162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

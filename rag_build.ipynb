{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caff47a",
   "metadata": {},
   "source": [
    "### This file creates embeddings based on the given text data and saves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "286e6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import hub\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621db037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents (chunks): 258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5877/2312775883.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758084220.223980    5877 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Document ---\n",
    "# Make sure your file path is correct\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Use a Glob pattern to load all .txt files from the 'content' directory\n",
    "loader = DirectoryLoader(\n",
    "    \"./content/\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "data = loader.load()\n",
    "\n",
    "# --- 2. Split the Document (with larger chunk size) ---\n",
    "# We use a larger chunk size to get more context per document chunk\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Total number of documents (chunks): {len(docs)}\")\n",
    "\n",
    "# --- 3. Create Embeddings and Vector Store ---\n",
    "# The embedding model converts text to numerical vectors\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store and a retriever that gets more documents\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "# We set 'k' to 10 to retrieve the top 10 most relevant documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# --- 4. Define the RAG Prompt and Language Model ---\n",
    "# We define a custom prompt to encourage a more detailed answer\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Be very detailed and comprehensive in your answer, providing a thorough summary based on the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "load_dotenv()\n",
    "# Initialize the Gemini Language Model\n",
    "# Note: You need to set up your Google API key in Colab secrets\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.7, google_api_key=os.environ.get('GOOGLE_API_KEY'))\n",
    "\n",
    "# --- 5. Build the RAG Chain ---\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895ea76",
   "metadata": {},
   "source": [
    "### Inferencing with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a852b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating summary...\n",
      "\n",
      "Based on the provided text, a direct comparison of the 2017-2021 settlement and a 2025 settlement is not possible.  The document details a settlement from 2017-2021 and then another settlement with a period from 2021-2026. There is no information about a separate settlement in 2025.\n",
      "\n",
      "However, we can compare the 2017-2021 settlement to the 2021-2026 settlement.  The major differences are:\n",
      "\n",
      "* **Duration:** The 2017-2021 settlement covered a period of 4 years (48 months), from December 1, 2017, to November 30, 2021.  The 2021-2026 settlement extends for 5 years, from December 1, 2021, to December 31, 2026.\n",
      "\n",
      "* **Wage Grade Correction and ABP Amount:** The 2021-2026 settlement includes specific clauses (11 and 12) addressing the correction of wage grades for G03 and G04 category permanent workmen to G07, and the correction of their ABP (Annual Benefit Payment) amounts from INR 4,080 to INR 5,818.  These corrections are not explicitly mentioned in the description of the 2017-2021 settlement.  While the 2017-2021 agreement *mentions* wage increases and adjustments, the specifics are not detailed in the provided text.\n",
      "\n",
      "* **Scope:** The 2017-2021 settlement explicitly applies to permanent workmen in salary/wage categories MC1 to MC6, MT1 to MT4, and G03 to G10, as of the signing date.  It also extends benefits (pro-rata) to those who expired, retired, or took early retirement between January 1, 2017, and the signing date (excluding NGO02).  The 2021-2026 settlement's scope is not explicitly defined in the same detail, but it does state that all other terms and conditions of the previous settlement remain unaltered, except for the specific clauses mentioned above.\n",
      "\n",
      "In summary, while the 2021-2026 settlement builds upon the 2017-2021 agreement, it introduces specific changes related to wage grade corrections and ABP amounts for G03 and G04 category workers, and extends the agreement's duration.  The provided text lacks sufficient detail to compare the overall financial implications or other nuanced changes between the two agreements beyond these key points.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Invoke the Chain ---\n",
    "# The final result will be a more detailed summary\n",
    "print(\"\\nGenerating summary...\\n\")\n",
    "summary_question = \"Compare settlement of 2017-2021 to 2025 what are major changes ?\"\n",
    "result = rag_chain.invoke(summary_question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059688a",
   "metadata": {},
   "source": [
    "### SAVE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b67ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving vector store...\n",
      "Vector store saved to 'vector_db'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5877/1660703030.py:35: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def save_vector_store(persist_directory=\"vector_db\"):\n",
    "    \"\"\"\n",
    "    Loads documents, creates embeddings, and saves the vector store to a directory.\n",
    "    \"\"\"\n",
    "    print(\"Saving vector store...\")\n",
    "    \n",
    "    # 1. Load documents\n",
    "    loader = DirectoryLoader(\n",
    "        \"./content/\",\n",
    "        glob=\"*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    data = loader.load()\n",
    "\n",
    "    # 2. Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    docs = text_splitter.split_documents(data)\n",
    "\n",
    "    # 3. Create embeddings\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 4. Create and persist the vector store\n",
    "    # This automatically saves the embeddings to the specified directory\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(f\"Vector store saved to '{persist_directory}'\")\n",
    "\n",
    "# Execute the function to save the vector store\n",
    "save_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5a0ed",
   "metadata": {},
   "source": [
    "### THIS CODE IS FOR INFERENCING AFTER SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acbfc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758092413.932134   42190 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# --- ONE-TIME SETUP ---\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Define the persistence directory for your vector store\n",
    "PERSIST_DIRECTORY = \"vector_db\"\n",
    "\n",
    "# 1. Initialize the embedding model (only once)\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Load the persisted vector store (only once)\n",
    "if not os.path.exists(PERSIST_DIRECTORY):\n",
    "    print(f\"Error: Vector store directory '{PERSIST_DIRECTORY}' not found.\")\n",
    "else:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "\n",
    "# 3. Create the retriever (only once)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 4. Define the RAG prompt (only once)\n",
    "template = \"\"\"\n",
    "You are a helpful assistant. Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Be very detailed and comprehensive in your answer, providing a detailed on the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# 5. Initialize the LLM (only once)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",\n",
    "    temperature=0.7,\n",
    "    google_api_key=os.environ.get('GOOGLE_API_KEY')\n",
    ")\n",
    "\n",
    "# 6. Build the RAG chain (only once)\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# --- INFERENCE FUNCTION ---\n",
    "\n",
    "def inference_with_rag(query):\n",
    "    \"\"\"\n",
    "    Performs a RAG query using the pre-loaded chain.\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuerying: '{query}'\")\n",
    "    if 'rag_chain' in globals():\n",
    "        result = rag_chain.invoke({\"input\": query})\n",
    "        return result['answer']\n",
    "    else:\n",
    "        return \"RAG chain not initialized. Check for errors during setup.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8ab18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Querying: 'What is the fixed basked amount for the year 2025'\n",
      "\n",
      "-------------------\n",
      "\n",
      "Answer: The provided text gives details of several fixed basket amounts across different years and different agreements.  There's no single \"fixed basket amount for the year 2025\" that applies universally. The amount depends on which agreement and which component of the fixed basket are being considered.\n",
      "\n",
      "Here's a breakdown based on the available information:\n",
      "\n",
      "\n",
      "**Agreement 1 (Page 15):** This section details a staggered payment over two years (2017-2018).  It does not extend to 2025.\n",
      "\n",
      "**Agreement 2 (Page 13-14):** This agreement covers the period from 01.12.2022 to 31.12.2026.  The total fixed basket amount is INR 9,450. However, this is distributed in a staggered manner:\n",
      "\n",
      "* 40% (INR 3780) from 01.12.2022 to 30.11.2023\n",
      "* 40% (INR 3780) from 01.12.2023 to 30.11.2024\n",
      "* 20% (INR 1890) from 01.12.2024 to 30.11.2025\n",
      "\n",
      "Therefore, for the year 2025 (01.12.2024 to 30.11.2025), 20% of the total fixed basket amount (INR 9450) will be paid, which is **INR 1890**.\n",
      "\n",
      "\n",
      "**Agreement 3 (Page 15-16):** This agreement discusses a fixed basket amount of INR 2677.50 per month, paid in a staggered manner over four years (2022-2025).  The breakdown is as follows:\n",
      "\n",
      "* Year 1 (to 30.11.2023): INR 409.50\n",
      "* Year 2 (to 30.11.2024): INR 1,756.00\n",
      "* Year 3 (to 30.11.2025): INR 540.00\n",
      "\n",
      "In this agreement, the total fixed basket payment for 2025 is **INR 540.00**.  Note that this is only a portion of the total increase over the four year period.\n",
      "\n",
      "**In summary:** There's no single answer.  The fixed basket amount for 2025 varies depending on which agreement is referenced.  Agreement 2 indicates INR 1890, while Agreement 3 shows INR 540.  More context is needed to determine which agreement is relevant to the specific question.\n",
      "\n",
      "-------------------\n",
      "\n",
      "\n",
      "Querying: 'How many years of settlement data do you have?'\n",
      "\n",
      "-------------------\n",
      "\n",
      "Answer: The provided text contains data for at least six different settlement periods:\n",
      "\n",
      "1. **01.01.2001 to 31.12.2004:** A four-year settlement covering workmen in categories G1 to G10, MC1 to MC6, and MT1 to MT4 at the Adugodi, Bangalore location.  This agreement included the full and final settlement of demands from a Charter of Demands dated 02.03.2001, and management proposals.  The agreement specifies that neither party can terminate it before its expiry.\n",
      "\n",
      "2. **01.01.2005 to 31.12.2008:** Another four-year settlement with the same worker categories and location as above.  This agreement also contained a full and final settlement of demands from a Charter of Demands dated 20.12.2004 and management proposals.  Similar termination clauses exist here.\n",
      "\n",
      "3. **01.01.2009 to 31.12.2012:**  A four-year settlement mirroring the previous two in terms of worker categories and location. This settlement also resolved demands from a Charter of Demands dated 22.05.2009 and management proposals.  It includes the same termination stipulations.\n",
      "\n",
      "4. **01.01.2013 to 31.12.2016:** A four-year settlement applicable to Permanent Workmen in categories MC1 to MC6, MT1 to MT4, and G03 to G10.  This agreement also extended benefits to those who expired, retired on attaining superannuation, or retired via Early Voluntary Retirement between 01.01.2013 and the signing of the MoS.  It specifically mentions the inclusion of workmen in the G02 category at the time of signing, with their subsequent promotion to G07 and reclassification as NGO2.\n",
      "\n",
      "5. **01.12.2017 to 30.11.2021:** A four-year (48-month) settlement for Permanent Workmen in categories MC1 to MC6, MT1 to MT4, and G03 to G10.  This settlement included provisions for ex-gratia payments (pro-rata for those separated between 01.01.2017 and 30.11.2017), and benefits extended to those who expired, retired, or took Early Voluntary Retirement between 01.01.2017 and the signing date.\n",
      "\n",
      "6. **01.12.2021 to 31.12.2026:** A five-year and one-month settlement for Permanent Workmen in categories G03 to G10.  This settlement also provided for ex-gratia payments (pro-rata for those separated between 01.12.2021 and the signing date) and benefits for those who expired, retired, or took Early Voluntary Retirement during the same period.  Page 14 mentions this settlement period again, stating the next settlement will be due from January 1st, 2027.\n",
      "\n",
      "\n",
      "Therefore, the provided text gives settlement data for a total of at least six different periods, spanning several years.  The exact number of years covered depends on the precise dates of signing each agreement, which are not explicitly stated in the provided excerpts.\n",
      "\n",
      "-------------------\n",
      "\n",
      "Exiting chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- EXAMPLE USAGE (looping for multiple queries) ---\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"\\nEnter your query (or type 'exit' to quit): \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        print(\"Exiting chatbot. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    response = inference_with_rag(user_query)\n",
    "    print(\"\\n-------------------\\n\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"\\n-------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64c162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
